\chapter{统计学习方法}
    \section{统计学习方法概述}
    统计学习方法的三要素

    \subsection{模型（model）}
    \subsection{策略（strategy）}
    \subsection{算法（algorithm）}    
    \subsection{模型评估}
    用已知预测未知（泛化能力）-训练误差与测试误差，过拟合、欠拟合

    奥卡姆剃刀

    当假设空间含有不同复杂度（例如，不同的参数个数）的模型时，就要面临模型选择(modelselection）的问题．我们希望选择或学习一个合适的模型．如果在假设空间中存在“真”模型，那么所选择的模型应该逼近真模型．具体地，所选择的模型要与真模型的参数个数相同，所选择的模型的参数向量与真模型的参数l向量相近.
    如果一味追求提高对训练数据的预测能力，所选模型的复杂度则往往会比真模型更高．这种现象称为过拟合（over-fiting)．过拟合是指学习时选择的模型所包含的参数过多，以致于出现这一模型对已知数据预测得很好，但对未知数据预测得很差的现象，可以说模型选择旨在避免\highunderline[yellow]{过拟合}并提高模型的预测能力.

    
    \subsection{有监督学习}
    有监督学习是一个函数，$f(x)=y$
    \subsubsection{分类}
    \subsubsection{标注}
    \subsubsection{回归}
    \subsubsection{生成模型和判别模型}
    \subsection{无监督学习}
    \subsection{半监督学习}
    \subsection{强化学习}
    \section{感知机}
    用平面二分类，样本点在平面两侧

    感知机的函数表示
    空间上的含义
    函数距离与几何距离
    函数与空间剖面的关系
    感知机的训练方法
    感知机的对偶形式及其训练方法
    拓展：感知机的收敛性
    拓展：感知机为什么不能表示异或
    \section{线性回归}
    用于拟合平面，样本点在平面附近
    \section{优化方法简述}
    感知机的优化方式
    \section{朴素贝叶斯}

    \subsection{后验概率最大化的含义}
    \begin{equation}
        \begin{aligned} 
            R_{e x p}(f)=E[L(Y, f(X))] &=\int_{X \times Y} L(y, f(x)) P(x, y) d x d y \\ &=\iint_{X \times Y} L(y, f(x)) P(y | x) P(x) d x d y \\ &=\int_{X}\left(\sum_{k=1}^{K} L\left(c_{k}, f(x)\right) P\left(y=c_{k} | x\right)\right) P(x) d(x) \\ &=E_{X} \sum_{k=1}^{K} L\left(c_{k}, f(X)\right) P\left(c_{k} | X\right) 
        \end{aligned}
    \end{equation}

    因此对所有的X=x逐个极小化
    \begin{equation}
        \begin{aligned}
            f(x) &=\underset{y \in Y}{\arg \min } \sum_{k=1}^{K} L\left(c_{k}, y\right) P\left(c_{k} | X=x\right) \\ &=\underset{y \in Y}{\arg \min } \sum_{k=1}^{K} P\left(y \neq c_{k} | X=x\right) \\ &=\underset{y \in Y}{\arg \min }\left(1-P\left(y=c_{k} | X=x\right)\right) \\ &=\underset{y \in Y}{\arg \max } P\left(y=c_{k} | X=x\right) 
        \end{aligned}
    \end{equation}

    对于不同的书，对于一些算法或算法行为有着不同的称谓，有的概念名称甚至连原书作者也拿捏不准，这也导致我们在初学翻阅各种资料时候发现一会儿又多了这个概念，一会儿又多了那个概念，及其痛苦。但是名称不重要，重要的是我们知道所指代的具体东西就行。下面就整理出笔者在学习中遇到过的各种“叫法”，仅供参考。
    
    \subsection{朴素贝叶斯的参数估计}
    \subsection{贝叶斯估计}
    贝叶斯估计实际上就是在朴素贝叶斯的基础上添加了一个平滑性,这种\textbf{估计}方法称为贝叶斯估计
    \begin{equation}
        P_{\lambda}\left(Y=c_{k}\right)=\frac{\sum_{i=1}^{m} I\left(y_{i}=c_{k}\right)+\lambda}{m+K \lambda}
    \end{equation}
    \begin{equation}
        P_{\lambda}\left(X^{(j)}=a_{j l} | Y=c_{k}\right)=\frac{\sum_{i=1}^{m} I\left(x_{i}^{(j)}=a_{j l}, y_{i}=c_{k}\right)+\lambda}{\sum_{i=1}^{m} I\left(y_{i}=c_{k}\right)+S_{j} \lambda}
    \end{equation}
    \subsection{贝叶斯决策}
    贝叶斯决策是一种统计决策理论，用于设计分类器，针对分类任务。朴素贝叶斯是基于贝叶斯决策理论,假设条件独立性后的一种具体的分类器算法.

    \subsection{贝叶斯网络}
    如果不假设条件独立性,而是认为条件之间存在概率依存关系,那么模型就变成了贝叶斯网络
    \section{逻辑回归}
    \subsection{线性回归}
    \subsection{二项回归、多项回归}
    \subsection{对分类条件概率建模}
    \subsection{激活函数}
    \section{广义线性模型}
    \subsection{指数分布族}

    \section{最大熵模型}
    \subsection{熵、信息熵、条件熵...}
    条件熵 H(Y|X)定义为 X 给定条件下 Y 的条件概率分布的熵对 X 的\textbf{数学期望}：
    \begin{equation}
        \begin{aligned} 
            H(Y | X) &=\sum_{x} p(x) H(Y | X=x) \\ 
                &=-\sum_{x} p(x) \sum_{y} p(y | x) \log p(y | x) \\
                &=-\sum_{x} \sum_{y} p(x, y) \log p(y | x) \\ 
                &=-\sum_{x, y} p(x, y) \log p(y | x) 
        \end{aligned}
    \end{equation}
    \subsection{最大熵原理及其解释}
    \subsubsection{证明均匀分布时熵最大}
    https://zhuanlan.zhihu.com/p/35379531
    \subsubsection{推导条件熵的公式}
    \begin{equation}
        \begin{aligned} 
            H(X, Y) &=-\sum_{x, y} p(x, y) \log p(x, y) \\ 
            &=-\sum_{x, y} p(x, y) \log (p(y | x) p(x)) \\ 
            &=-\sum_{x, y} p(x, y) \log p(y | x)-\sum_{x, y} p(x, y) \log p(x) \\ 
            &=H(Y | X)-\sum_{x, y} p(x, y) \log p(x) \\
            &=H(Y | X)-\sum_{x} \sum_{y} p(x, y) \log p(x) \\ &=H(Y | X)-\sum_{x} \log p(x) \sum_{y} p(x, y) \\ 
            &=H(Y | X)-\sum_{x}(\log p(x)) p(x) \\ 
            &=H(Y | X)-\sum_{x} p(x) \log p(x) \\ 
            &=H(Y | X)+H(X)
        \end{aligned}
    \end{equation}
    \subsection{最大熵模型中的特征函数}
    最大熵模型中的每个特征会有一个权重，你可以把它理解成这个特征所描述的输入和输出有多么倾向于同时出现。由于特征函数是人为定义的，因此可以理解为，通过人的常识，认为添加的对模型的约束条件？
    \subsection{最大熵模型的优化}
    \subsubsection{最优化问题的模型表示}
    引入算子，表示为拉格朗日函数，介绍凸函数，KKT条件

    % 最完整推导：https://shuwoom.com/?p=1168
    \subsubsection{对偶函数和极大似然估计形式的等价性}
    对偶函数$\psi(w)$的推导%from https://www.jianshu.com/p/10d778068f70 ,有点推不动...
    \begin{equation}
        \begin{aligned}
            \psi(w)&=\min _{p \in C} L(P, w) \\
            &=\sum_{x, y} \tilde{P}(\mathrm{x})+ \mathrm{P}_{\mathrm{w}}(\mathrm{y} | \mathrm{x}) \log P_{\mathrm{w}}(y | x)+w_{0}\left(1-\sum_{y} P_{w}(y | x)\right)\\
                &\qquad +\sum_{i=1}^{n} w_{i}\left(\sum_{x y} \tilde{P}(x, y) f_{i}(x, y)-\sum_{x y} \mathrm{P}(\mathrm{x}) \mathrm{P}_{w}(\mathrm{y} | \mathrm{x}) f_{i}(x, y)\right)\\
            &=\sum_{x, y} \mathrm{P}(\mathrm{x}) \mathrm{P}_{\mathrm{w}}(\mathrm{y} | \mathrm{x}) \log P_{\mathrm{w}}(y | x)\\
                &\qquad +\sum_{\mathrm{i}=1}^{n} w_{i}\left(\sum_{x, y} \tilde{P}(x, y) f_{i}(x, y)-\sum_{x, y} \mathrm{P}(\mathrm{x}) \mathrm{P}_{w}(\mathrm{y} | \mathrm{x}) f_{i}(x, y)\right)\\
            &=\sum_{x, y} \sum_{i=1}^{n} w_{i} \tilde{P}(x, y) f_{i}(x, y)\\
                &\qquad +\sum_{x, y} \tilde{P}(\mathrm{x}) \mathrm{P}_{\mathrm{w}}(\mathrm{y} | \mathrm{x})\left[\log P_{\mathrm{w}}(y | x)-\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right]\\
            &=\sum_{x, y} \sum_{i=1}^{n} w_{i} \tilde{P}(x, y) f_{i}(x, y)-\sum_{x, y} \tilde{P}(\mathrm{x}) \mathrm{P}_{\mathrm{w}}(\mathrm{y} | \mathrm{x}) \log Z_{w}(x)\\
            &=\sum_{x, y} \sum_{i=1}^{n} w_{i} \tilde{P}(x, y) f_{i}(x, y)-\sum_{x} \tilde{\mathrm{P}}(\mathrm{x}) \log Z_{w}(x)
        \end{aligned}
    \end{equation}
    
    \subsection{最大熵模型和逻辑回归的等价性}
    %https://www.zhihu.com/question/24094554/answer/108271031
    \subsection{实例}
    % http://www.huaxiaozhuan.com/统计学习/chapters/14_maxent.html
    
    
    \section{支持向量机}
    \subsection{正则化-岭回归与Lasso回归}
    正则化项本质上是一种先验信息，整个最优化问题从贝叶斯观点来看是一种贝叶斯最大后验估计，其中正则化项对应后验估计中的先验信息，损失函数对应后验估计中的似然函数，两者的乘积即对应贝叶斯最大后验估计的形式，如果你将这个贝叶斯最大后验估计的形式取对数，即进行极大似然估计，你就会发现问题立马变成了损失函数+正则化项的最优化问题形式
    
    支持向量机(SVM)是90年代中期发展起来的基于统计学习理论的一种机器学习方法，通过寻求结构化风险最小来提高学习机泛化能力，实现经验风险和置信范围的最小化，从而达到在统计样本量较少的情况下，亦能获得良好统计规律的目的。
    
    核方法和核函数
    \section{序列建模}
    \subsection{隐马尔科夫模型}
    \subsection{最大熵马尔可夫模型}
    \subsection{条件随机场}
    \subsection{结构化感知机}
    \subsection{总结}
    这四种模型的区别，各自的优缺点
    \section{K近邻}
    \section{决策树}
    \section{优化方法}
    \subsection{牛顿法和拟牛顿法}
    \subsection{迭代尺度法}
    \subsection{梯度下降法}